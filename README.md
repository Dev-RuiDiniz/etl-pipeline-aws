# ğŸš€ Pipeline ETL Automatizado â€” AWS + Python

Pipeline ETL serverless desenvolvido para extrair dados de uma API pÃºblica, transformar com Pandas e carregar no PostgreSQL (AWS RDS).  
Arquivos RAW, processados e logs sÃ£o armazenados no Amazon S3, com orquestraÃ§Ã£o por EventBridge e execuÃ§Ã£o via Lambda.

Este projeto demonstra arquitetura moderna de dados, boas prÃ¡ticas de engenharia, testes automatizados, infraestrutura como cÃ³digo e escalabilidade utilizando AWS.

---

## ğŸ§© **Principais Tecnologias**

- **Python 3.11**
- **AWS Lambda**
- **Amazon S3**
- **Amazon RDS (PostgreSQL)**
- **AWS EventBridge**
- **Pandas**
- **SQLAlchemy**
- **Boto3**
- **Pydantic BaseSettings**
- **Pytest**
- **AWS CDK / SAM / Serverless Framework**

---

## ğŸ“ **Arquitetura do Pipeline**

(C:\Users\RUI FRANCISCO\Downloads\mermaid.png)

---

## ğŸ“ **Estrutura do Projeto**

```
etl-pipeline-aws/
â”‚
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ extract/
â”‚   â”‚   â””â”€â”€ extractor.py
â”‚   â”œâ”€â”€ transform/
â”‚   â”‚   â””â”€â”€ transformer.py
â”‚   â”œâ”€â”€ load/
â”‚   â”‚   â””â”€â”€ loader.py
â”‚   â”œâ”€â”€ utils/
â”‚   â”‚   â”œâ”€â”€ logger.py
â”‚   â”‚   â””â”€â”€ aws.py
â”‚   â”œâ”€â”€ main.py
â”‚
â”œâ”€â”€ config/
â”‚   â””â”€â”€ settings.py
â”‚
â”œâ”€â”€ tests/
â”‚   â”œâ”€â”€ test_extractor.py
â”‚   â”œâ”€â”€ test_transformer.py
â”‚   â””â”€â”€ test_loader.py
â”‚
â”œâ”€â”€ infra/
â”‚   â”œâ”€â”€ cdk/
â”‚   â”œâ”€â”€ sam/
â”‚   â””â”€â”€ serverless/
â”‚
â”œâ”€â”€ Dockerfile
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ .env
â”œâ”€â”€ .gitignore
â””â”€â”€ README.md

```

---

## âš™ï¸ **ConfiguraÃ§Ã£o do Ambiente**

Crie o arquivo .env na raiz do projeto:
```
ini

API_URL=https://sua-api-publica.com/data
DB_HOST=xxx.amazonaws.com
DB_USER=postgres
DB_PASSWORD=SENHA
DB_NAME=etl_database
DB_PORT=5432

S3_BUCKET=meu-bucket-etl
AWS_REGION=us-east-1
```
O carregamento das variÃ¡veis Ã© feito por Pydantic em config/settings.py.

---

## â–¶ï¸ **ExecuÃ§Ã£o Local**
1. Criar ambiente virtual
```
bash
python -m venv venv
source venv/bin/activate  # Linux/Mac
venv\Scripts\activate     # Windows
```
2. Instalar dependÃªncias
```
bash
pip install -r requirements.txt
```
3. Rodar o pipeline localmente
```
bash
python src/main.py
```
---
## ğŸ§ª **Testes Automatizados (pytest)**
```
bash
pytest -q
```
Testes cobrem:
- ExtraÃ§Ã£o da API (mock requests)
- TransformaÃ§Ã£o com Pandas
- ConexÃ£o com banco de dados

---

## ğŸ³ **Executando com Docker**
Build:
```
bash
docker build -t etl-pipeline .
```
Rodar:
```
bash
docker run --env-file .env etl-pipeline
```
---
## â˜ï¸ **Deploy na AWS**
Escolha sua ferramenta preferida:

- ğŸš€ OpÃ§Ã£o 1 â€” AWS CDK (recomendado)
Instale o CDK:
```
bash
npm install -g aws-cdk
pip install aws-cdk-lib constructs
```
Bootstrap:
```
bash
cd infra/cdk
cdk bootstrap
```
Gerar template:
```
bash
cdk synth
```
Deploy:
```
bash
cdk deploy
```
- ğŸš€ OpÃ§Ã£o 2 â€” AWS SAM
```
bash
cd infra/sam
sam build
sam deploy --guided
```
- ğŸš€ OpÃ§Ã£o 3 â€” Serverless Framework
```
bash
cd infra/serverless
serverless deploy
```
---
## ğŸ“Œ **Principais Funcionalidades**

- ExtraÃ§Ã£o robusta com retry e tratamento de erros
- TransformaÃ§Ã£o com Pandas
- Salvamento de arquivos RAW e processados no S3
- Carregamento em PostgreSQL (RDS)
- Logs estruturados no CloudWatch
- Testes unitÃ¡rios
- Deploy via IaC (CDK / SAM / Serverless)
- Dockerfile para execuÃ§Ã£o local
---
## ğŸ§  **PrÃ¡ticas Adotadas de Engenharia de Dados**

- Arquitetura Clean (extract/transform/load separados)
- IdempotÃªncia no processamento (drop_duplicates)
- Versionamento de dados no S3 com timestamps
- SegregaÃ§Ã£o de ambiente via .env e Pydantic
- Testes com mocks para chamadas externas
- Observabilidade (CloudWatch + logs estruturados)
- Templates de IaC
---
## ğŸ“„ **LicenÃ§a**

Este projeto Ã© distribuÃ­do sob a licenÃ§a MIT.
Sinta-se livre para usar, modificar e evoluir.

---
## ğŸ“¬ **Contato**
### **Desenvolvido por Rui Francisco de Paula InÃ¡cio Diniz**

**_Engenheiro de Software â€¢ Desenvolvedor Python â€¢ Analista de Dados_**
